{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Making a link list\n",
    "Create a link list containing urls to the nutrition data and save to links.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from external packages\n",
    "import requests \n",
    "import bs4 as bs \n",
    "import time, json, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arby's https://fastfoodnutrition.org/arbys\n",
      "Baskin-Robbins https://fastfoodnutrition.org/baskin-robbins\n",
      "Blimpie https://fastfoodnutrition.org/blimpie\n",
      "Boston Market https://fastfoodnutrition.org/boston-market\n",
      "Buffalo Wild Wings https://fastfoodnutrition.org/buffalo-wild-wings\n",
      "Burger King https://fastfoodnutrition.org/burger-king\n",
      "Carl's Jr https://fastfoodnutrition.org/carls-jr\n",
      "Chick-fil-A https://fastfoodnutrition.org/chick-fil-a\n",
      "Chipotle https://fastfoodnutrition.org/chipotle\n",
      "Culvers https://fastfoodnutrition.org/culvers\n",
      "Dairy Queen https://fastfoodnutrition.org/dairy-queen\n",
      "Del Taco https://fastfoodnutrition.org/del-taco\n",
      "Domino's Pizza https://fastfoodnutrition.org/dominos-pizza\n",
      "Dunkin Donuts https://fastfoodnutrition.org/dunkin-donuts\n",
      "Five Guys https://fastfoodnutrition.org/five-guys\n",
      "Godfather's Pizza https://fastfoodnutrition.org/godfathers-pizza\n",
      "Hardee's https://fastfoodnutrition.org/hardees\n",
      "In-N-Out Burger https://fastfoodnutrition.org/in-n-out-burger\n",
      "Jack in the Box https://fastfoodnutrition.org/jack-in-the-box\n",
      "Jimmy Johns https://fastfoodnutrition.org/jimmy-johns\n",
      "KFC https://fastfoodnutrition.org/kfc\n",
      "Little Caesars https://fastfoodnutrition.org/little-caesars\n",
      "Long John Silver's https://fastfoodnutrition.org/long-john-silvers\n",
      "McDonald's https://fastfoodnutrition.org/mcdonalds\n",
      "Olive Garden https://fastfoodnutrition.org/olive-garden\n",
      "Panda Express https://fastfoodnutrition.org/panda-express\n",
      "Panera https://fastfoodnutrition.org/panera-bread\n",
      "Papa John's https://fastfoodnutrition.org/papa-johns\n",
      "Pizza Hut https://fastfoodnutrition.org/pizza-hut\n",
      "Popeyes https://fastfoodnutrition.org/popeyes\n",
      "Quiznos https://fastfoodnutrition.org/quiznos\n",
      "Red Lobster https://fastfoodnutrition.org/red-lobster\n",
      "Smashburger https://fastfoodnutrition.org/smashburger\n",
      "Sonic https://fastfoodnutrition.org/sonic\n",
      "Starbucks https://fastfoodnutrition.org/starbucks\n",
      "Subway https://fastfoodnutrition.org/subway\n",
      "Taco Bell https://fastfoodnutrition.org/taco-bell\n",
      "Taco John's https://fastfoodnutrition.org/taco-johns\n",
      "Wendy's https://fastfoodnutrition.org/wendys\n",
      "Whataburger https://fastfoodnutrition.org/whataburger\n",
      "Zaxby's https://fastfoodnutrition.org/zaxbys\n"
     ]
    }
   ],
   "source": [
    "# get the links for the recipes from API source\n",
    "link=[]\n",
    "brand=[]\n",
    "\n",
    "\n",
    "#get restaurant list\n",
    "\n",
    "def extract_source(url): #the page requires user agent to access, otherwise return no access, so use this function\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'}\n",
    "    source=requests.get(url, headers=headers)\n",
    "    source.encoding = 'utf-8'\n",
    "    return source.content\n",
    "\n",
    "source = extract_source(\"https://fastfoodnutrition.org/\")\n",
    "soup = bs.BeautifulSoup(source , features='html.parser')  \n",
    "restaurant_list= soup.select('.restaurant_list')[0]\n",
    "for a in restaurant_list.find_all('a',attrs={'class':'c_t'}):\n",
    "    link.append(\"https://fastfoodnutrition.org\"+a['href'])\n",
    "    brand.append(a['title'].replace(\" Nutrition Facts\",\"\").strip())\n",
    "    #url = a['href']\n",
    "\n",
    "i=0\n",
    "for x in link:\n",
    "    print (brand[i],x)\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_link=[]\n",
    "\n",
    "def get_food_links_more(url, cat,brand): #not used in here, has improved deletected in crwaling process\n",
    "    print(\"crawling (expanded) ...\"+url)\n",
    "    source = extract_source(url)\n",
    "    soup = bs.BeautifulSoup(source , features='html.parser')  \n",
    "    food_list= soup.select('.stub_box')\n",
    "    for a in food_list:\n",
    "        food_name.append(a['title'].replace(\" Nutrition Facts\",\"\").strip())\n",
    "        food_link.append(\"https://fastfoodnutrition.org\"+a['href'].strip())\n",
    "        food_brand.append(brand)\n",
    "        food_cat.append(cat)\n",
    "    time.sleep(random.uniform(0.2,2.2)) #wait random time to go to next page\n",
    "\n",
    "def get_food_links(i):\n",
    "    print(\"crawling...\"+link[i])\n",
    "    source = extract_source(link[i])\n",
    "    soup = bs.BeautifulSoup(source , features='html.parser')  \n",
    "    food_list= soup.select('.category')\n",
    "    for cat_item in food_list:\n",
    "        cat=(cat_item.h2.get_text().strip())\n",
    "        for a in cat_item.ul.find_all('a',attrs={'class':'listlink'}):\n",
    "            food_link.append(\"https://fastfoodnutrition.org\"+a['href'].strip())\n",
    "                \n",
    "    time.sleep(random.uniform(1.2,3.6)) #wait random time to go to next page\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling...https://fastfoodnutrition.org/arbys\n",
      "crawling...https://fastfoodnutrition.org/baskin-robbins\n",
      "crawling...https://fastfoodnutrition.org/blimpie\n",
      "crawling...https://fastfoodnutrition.org/boston-market\n",
      "crawling...https://fastfoodnutrition.org/buffalo-wild-wings\n",
      "crawling...https://fastfoodnutrition.org/burger-king\n",
      "crawling...https://fastfoodnutrition.org/carls-jr\n",
      "crawling...https://fastfoodnutrition.org/chick-fil-a\n",
      "crawling...https://fastfoodnutrition.org/chipotle\n",
      "crawling...https://fastfoodnutrition.org/culvers\n",
      "crawling...https://fastfoodnutrition.org/dairy-queen\n",
      "crawling...https://fastfoodnutrition.org/del-taco\n",
      "crawling...https://fastfoodnutrition.org/dominos-pizza\n",
      "crawling...https://fastfoodnutrition.org/dunkin-donuts\n",
      "crawling...https://fastfoodnutrition.org/five-guys\n",
      "crawling...https://fastfoodnutrition.org/godfathers-pizza\n",
      "crawling...https://fastfoodnutrition.org/hardees\n",
      "crawling...https://fastfoodnutrition.org/in-n-out-burger\n",
      "crawling...https://fastfoodnutrition.org/jack-in-the-box\n",
      "crawling...https://fastfoodnutrition.org/jimmy-johns\n",
      "crawling...https://fastfoodnutrition.org/kfc\n",
      "crawling...https://fastfoodnutrition.org/little-caesars\n",
      "crawling...https://fastfoodnutrition.org/long-john-silvers\n",
      "crawling...https://fastfoodnutrition.org/mcdonalds\n",
      "crawling...https://fastfoodnutrition.org/olive-garden\n",
      "crawling...https://fastfoodnutrition.org/panda-express\n",
      "crawling...https://fastfoodnutrition.org/panera-bread\n",
      "crawling...https://fastfoodnutrition.org/papa-johns\n",
      "crawling...https://fastfoodnutrition.org/pizza-hut\n",
      "crawling...https://fastfoodnutrition.org/popeyes\n",
      "crawling...https://fastfoodnutrition.org/quiznos\n",
      "crawling...https://fastfoodnutrition.org/red-lobster\n",
      "crawling...https://fastfoodnutrition.org/smashburger\n",
      "crawling...https://fastfoodnutrition.org/sonic\n",
      "crawling...https://fastfoodnutrition.org/starbucks\n",
      "crawling...https://fastfoodnutrition.org/subway\n",
      "crawling...https://fastfoodnutrition.org/taco-bell\n",
      "crawling...https://fastfoodnutrition.org/taco-johns\n",
      "crawling...https://fastfoodnutrition.org/wendys\n",
      "crawling...https://fastfoodnutrition.org/whataburger\n",
      "crawling...https://fastfoodnutrition.org/zaxbys\n",
      "Crawling links complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://fastfoodnutrition.org/arbys/arbys-melt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://fastfoodnutrition.org/arbys/arby-q-san...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://fastfoodnutrition.org/arbys/beef-n-che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://fastfoodnutrition.org/arbys/beer-batte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://fastfoodnutrition.org/arbys/buttermilk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>https://fastfoodnutrition.org/zaxbys/fried-pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4149</th>\n",
       "      <td>https://fastfoodnutrition.org/zaxbys/fried-whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4150</th>\n",
       "      <td>https://fastfoodnutrition.org/zaxbys/onion-rings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>https://fastfoodnutrition.org/zaxbys/spicy-fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>https://fastfoodnutrition.org/zaxbys/tater-chips</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4153 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   link\n",
       "0        https://fastfoodnutrition.org/arbys/arbys-melt\n",
       "1     https://fastfoodnutrition.org/arbys/arby-q-san...\n",
       "2     https://fastfoodnutrition.org/arbys/beef-n-che...\n",
       "3     https://fastfoodnutrition.org/arbys/beer-batte...\n",
       "4     https://fastfoodnutrition.org/arbys/buttermilk...\n",
       "...                                                 ...\n",
       "4148  https://fastfoodnutrition.org/zaxbys/fried-pic...\n",
       "4149  https://fastfoodnutrition.org/zaxbys/fried-whi...\n",
       "4150   https://fastfoodnutrition.org/zaxbys/onion-rings\n",
       "4151  https://fastfoodnutrition.org/zaxbys/spicy-fri...\n",
       "4152   https://fastfoodnutrition.org/zaxbys/tater-chips\n",
       "\n",
       "[4153 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in range(len(link)):\n",
    "    get_food_links(x)\n",
    "\n",
    "print(\"Crawling links complete\")\n",
    "    \n",
    "import pandas as pd \n",
    "import csv \n",
    "\n",
    "dictionary={\n",
    "    'link':food_link\n",
    "}\n",
    "df = pd.DataFrame.from_dict(dictionary, orient ='index').transpose()\n",
    "\n",
    "df.to_csv(\"links.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2 Get nutrition data from links.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import bs4 as bs \n",
    "import time, json, random\n",
    "import pandas as pd \n",
    "import csv \n",
    "data = pd.read_csv(\"links.csv\")\n",
    "datasrc=data[\"link\"]\n",
    "\n",
    "\n",
    "Brand=[]\n",
    "Product=[]\n",
    "Link=[]\n",
    "Category=[]\n",
    "\n",
    "\n",
    "Serving_Size=[]\n",
    "Calories=[]\n",
    "Calories_From_Fat=[]\n",
    "Calories_From_Protein=[]\n",
    "Calories_From_Carb=[]\n",
    "\n",
    "Total_Fat=[]\n",
    "Saturated_Fat=[]\n",
    "Trans_Fat=[]\n",
    "Cholesterol=[]\n",
    "Sodium=[]\n",
    "Total_Carbohydrates=[]\n",
    "Dietary_Fiber=[]\n",
    "Sugars=[]\n",
    "Protein=[]\n",
    "#PDV= percent daily value\n",
    "Total_Fat_PDV=[]\n",
    "Saturated_Fat_PDV=[]\n",
    "Cholesterol_PDV=[]\n",
    "Sodium_PDV=[]\n",
    "Total_Carbohydrates_PDV=[]\n",
    "Dietary_Fiber_PDV=[]\n",
    "Protein_PDV=[]\n",
    "Vitamin_A_PDV=[]\n",
    "Vitamin_C_PDV=[]\n",
    "Calcium_PDV=[]\n",
    "Iron_PDV=[]\n",
    "\n",
    "Ingredients=[]\n",
    "Health_Rating=[]\n",
    "Taste_Rating=[]\n",
    "def extract_source(url): #the page requires user agent to access, otherwise return no access, so use this function\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'}\n",
    "    source=requests.get(url, headers=headers)\n",
    "    source.encoding = 'utf-8'\n",
    "    return source.content\n",
    "\n",
    "\n",
    "def get_nutri_data(soup):\n",
    "    nutri=dict()\n",
    "    pdv=dict() #percent daily value\n",
    "    \n",
    "    if not check_valid_page(soup): #checks is normal data page\n",
    "        print('** skipping non-nutri info or missing data page')\n",
    "        return\n",
    "        \n",
    "    def nutri_lookup(type_,keyword, unit): #look up by nutiriton item name, and return value; type_: nutri /pdv\n",
    "        try:\n",
    "            if type_==\"nutri\":\n",
    "                if unit!=\"\":\n",
    "                    return float(nutri[keyword].replace(unit,''))\n",
    "                else:\n",
    "                    return float(nutri[keyword])\n",
    "            else:\n",
    "                return float(pdv[keyword])\n",
    "        except:\n",
    "            print(\"error: has no data (\"+type_+\")-\"+keyword)\n",
    "            return -9999.0\n",
    "\n",
    "    for x in soup.select('tr'):\n",
    "        key=x.select('td')[0].get_text(strip=True)\n",
    "        val1=x.select('td')[1].get_text(strip=True)\n",
    "        try: #some row only have 2 columns\n",
    "            val2=x.select('td')[2].get_text(strip=True).replace(\"%\",\"\")\n",
    "            if val2==\"?\":\n",
    "                val2=\"\"\n",
    "        except:\n",
    "            val2=\"\"\n",
    "        if (\"Serving Size\" in key):\n",
    "            nutri[\"Serving Size\"]=x.select('td')[1].get_text(strip=True)\n",
    "        elif (\"Calories From Fat\" in  key):\n",
    "            nutri[\"Calories From Fat\"]=val1\n",
    "        elif (\"Calories\" in  key):\n",
    "            nutri[\"Calories\"]=val1\n",
    "        elif key!=\"\" :\n",
    "            nutri[key]=val1\n",
    "            pdv[key]=val2\n",
    "            \n",
    "    healthRating=\"-9999\"\n",
    "    healthyRatingText= soup.select('.ratingbarbg')[0]['title']\n",
    "    try:\n",
    "        if(\"healthy\" in healthyRatingText): #double check\n",
    "            healthRating=int(healthyRatingText.split(\"%\")[0].strip())\n",
    "    except:\n",
    "        healthRating=\"-9999\"\n",
    "        \n",
    "    tastyRating=\"-9999\"\n",
    "    tastyRatingText= soup.select('.ratingbarbg')[1]['title']\n",
    "    try:\n",
    "        if(\"tastes good\" in tastyRatingText or \"taste good\" in tastyRatingText): #double check\n",
    "            tastyRating=int(tastyRatingText.split(\"%\")[0].strip())\n",
    "    except:\n",
    "        tastyRating=\"-9999\"\n",
    "    Health_Rating.append(healthRating) \n",
    "    Taste_Rating.append(tastyRating)\n",
    "\n",
    "    \n",
    "    Serving_Size.append(nutri[\"Serving Size\"])\n",
    "    Calories.append(nutri[\"Calories\"])\n",
    "    #Calories_From_Fat.append(nutri[\"Calories From Fat\"])\n",
    "\n",
    "    Total_Fat.append(nutri_lookup(\"nutri\",\"Total Fat\",\"g\"))\n",
    "    Saturated_Fat.append(nutri_lookup(\"nutri\",\"Saturated Fat\",\"g\"))\n",
    "    Trans_Fat.append(nutri_lookup(\"nutri\",\"Trans Fat\",\"g\"))\n",
    "    Cholesterol.append(nutri_lookup(\"nutri\",\"Cholesterol\",\"mg\"))\n",
    "    Sodium.append(nutri_lookup(\"nutri\",\"Sodium\",\"mg\"))\n",
    "    Total_Carbohydrates.append(nutri_lookup(\"nutri\",\"Total Carbohydrates\",\"g\"))\n",
    "    Dietary_Fiber.append(nutri_lookup(\"nutri\",\"Dietary Fiber\",\"g\"))\n",
    "    Sugars.append(nutri_lookup(\"nutri\",\"Sugars\",\"g\"))\n",
    "    Protein.append(nutri_lookup(\"nutri\",\"Protein\",\"g\"))\n",
    "\n",
    "    Total_Fat_PDV.append(nutri_lookup(\"pdv\",\"Total Fat\",\"g\"))\n",
    "    Saturated_Fat_PDV.append(nutri_lookup(\"pdv\",\"Saturated Fat\",\"g\"))\n",
    "    Cholesterol_PDV.append(nutri_lookup(\"pdv\",\"Cholesterol\",\"mg\"))\n",
    "    Sodium_PDV.append(nutri_lookup(\"pdv\",\"Sodium\",\"mg\"))\n",
    "    Total_Carbohydrates_PDV.append(nutri_lookup(\"pdv\",\"Total Carbohydrates\",\"g\"))\n",
    "    Dietary_Fiber_PDV.append(nutri_lookup(\"pdv\",\"Dietary Fiber\",\"g\"))\n",
    "    Protein_PDV.append(nutri_lookup(\"pdv\",\"Protein\",\"g\"))\n",
    "    Vitamin_A_PDV.append(nutri_lookup(\"pdv\",\"Vitamin A\",\"g\"))\n",
    "    Vitamin_C_PDV.append(nutri_lookup(\"pdv\",\"Vitamin C\",\"g\"))\n",
    "    Calcium_PDV.append(nutri_lookup(\"pdv\",\"Calcium\",\"g\"))\n",
    "    Iron_PDV.append(nutri_lookup(\"pdv\",\"Iron\",\"g\"))    \n",
    "    \n",
    "    temp=soup.select('#ingredients')[0].get_text().split(\"Ingredients\")\n",
    "    temp[0]=\"\" #remove first item, usually repeated name of the product\n",
    "    Ingredients.append(\" \".join(temp).strip())\n",
    "    temp=str(soup).split(\"data:\") #get protein-fat-carb pie chart data from JS\n",
    "    temp=temp[1].split(\"]\")[0].replace(\"[\",\"\")\n",
    "    temp=temp.split(\",\")\n",
    "    if float(nutri[\"Calories\"])==0.0: #zero calories item\n",
    "        Calories_From_Protein.append(0.0)\n",
    "        Calories_From_Fat.append(0.0)\n",
    "        Calories_From_Carb.append(0.0)\n",
    "    else:\n",
    "        hasError=False\n",
    "        try: #normal data\n",
    "            p= float(temp[0].strip())\n",
    "            f= float(temp[1].strip())\n",
    "            c= float(temp[2].strip())\n",
    "        except: #cannot find data\n",
    "            hasError=True\n",
    "        if hasError:\n",
    "            Calories_From_Protein.append(-9999)\n",
    "            Calories_From_Fat.append(-9999)\n",
    "            Calories_From_Carb.append(-9999)\n",
    "        else:\n",
    "            Calories_From_Protein.append(p)\n",
    "            Calories_From_Fat.append(f)\n",
    "            Calories_From_Carb.append(c)\n",
    "            \n",
    "    \n",
    "    #breadcrumb structure: [0]Restaurent> [1]Brand> [2]Category> [3]Prodcut\n",
    "    # usually it has <a> element in breadcrumb, but the last one may be <a> or <span>\n",
    "    breadcrumb=soup.find_all('a',attrs={'breadcrumb-item'})\n",
    "    breadcrumb2=soup.find_all('span',attrs={'breadcrumb-item'})\n",
    "    brand_=breadcrumb[1].get_text()\n",
    "    cat_=breadcrumb[2].get_text()\n",
    "    if len(breadcrumb)==3 :  #\"normal one\"\n",
    "        try:\n",
    "            product_=breadcrumb2[0].get_text()\n",
    "        except:\n",
    "            product_=breadcrumb[len(breadcrumb)-1].get_text()\n",
    "    else: #\"size\" = breadcrumb[3]+last(Size)\n",
    "        try:\n",
    "            product_=breadcrumb[len(breadcrumb)-1].get_text()+\" \"+breadcrumb2[0].get_text()\n",
    "        except:\n",
    "            product_=breadcrumb[len(breadcrumb)-1].get_text()\n",
    "    Brand.append(brand_)\n",
    "    Product.append(product_)\n",
    "    Category.append(cat_)\n",
    "    l=(soup.find(\"link\", {\"rel\":\"canonical\"})['href'])\n",
    "    Link.append(l)\n",
    "    print(\"Success crawl... \"+l)\n",
    "\n",
    "    time.sleep(random.uniform(2.0,3.6)) #wait random time to go to next page\n",
    "\n",
    "\n",
    "def check_has_more_links(url): #some pages may be \"index\" to various sizes of the item,\n",
    "    #function returns True if discovers more, and return list of links\n",
    "    #function returns False if it is the actual nutri. page, and return the html\n",
    "    result=dict()\n",
    "    source = extract_source(url)\n",
    "    soup = bs.BeautifulSoup(source , features='html.parser')  \n",
    "    food_list= soup.select('.stub_box')\n",
    "    if len(food_list)==0: #it is actual nutri page\n",
    "        result[0]=False\n",
    "        result['soup']=soup\n",
    "    else:\n",
    "        print(\"Further product list discovered- length:\"+str(len(food_list)))\n",
    "        result[0]=True\n",
    "        result['more_url']=list()\n",
    "        for a in food_list:\n",
    "            result['more_url'].append(\"https://fastfoodnutrition.org\"+a['href'].strip())\n",
    "    return result\n",
    "\n",
    "def check_valid_page(soup): # a correct page should contain 4 items in .breadcrumb-item (a or sapn)\n",
    "    stub_box= soup.select('.stub_box') #checks if this page is links to other sizes, and exit\n",
    "    if len(stub_box)>0: #contiain links to other sizes exit\n",
    "        return False\n",
    "    extraList=soup.find_all(\"ul\", {\"class\": \"rest_item_list\"}) #skip startbucks choose milk pages, with extra links\n",
    "    if len(extraList)>0 :\n",
    "        return False\n",
    "    \n",
    "    errorMessage=soup.find_all(\"div\", {\"class\": \"alert-danger\"})\n",
    "    if len(errorMessage)>0 :\n",
    "        return False\n",
    "    breadcrumb=soup.find_all('a',attrs={'breadcrumb-item'})\n",
    "    breadcrumb2=soup.find_all('span',attrs={'breadcrumb-item'})\n",
    "    if (len(breadcrumb)+len(breadcrumb2))<4:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def scrap_data(start,end):\n",
    "    #for x in range(len(datasrc)):\n",
    "    for x in range(start,end):\n",
    "        print(\"Crawling [\"+str(x)+\"]\"+datasrc[x])\n",
    "        res=check_has_more_links(datasrc[x]) #checks if the link is a page with links to other sizes\n",
    "        if res[0]: #has more links\n",
    "            for u in res['more_url']:\n",
    "                source = extract_source(u)\n",
    "                soup = bs.BeautifulSoup(source , features='html.parser')\n",
    "                get_nutri_data(soup)\n",
    "                time.sleep(random.uniform(2.0,3.6)) #wait random time to go to next page\n",
    "        else: #'direct' nutri page\n",
    "            if not check_valid_page(res['soup']):\n",
    "                print(\"** Skipping invalid page\")\n",
    "                continue\n",
    "            get_nutri_data(res['soup'])\n",
    "\n",
    "    dictionary={\n",
    "        \"Brand\":Brand,\n",
    "        \"Product\":Product,\n",
    "        \"Link\":Link,\n",
    "        \"Category\":Category,\n",
    "        \"Serving_Size\":Serving_Size,\n",
    "        \"Calories\":Calories,\n",
    "        \"Calories_From_Fat\":Calories_From_Fat,\n",
    "        \"Calories_From_Protein\":Calories_From_Protein,\n",
    "        \"Calories_From_Carb\":Calories_From_Carb,\n",
    "        \"Total_Fat\":Total_Fat,\n",
    "        \"Saturated_Fat\":Saturated_Fat,\n",
    "        \"Trans_Fat\":Trans_Fat,\n",
    "        \"Cholesterol\":Cholesterol,\n",
    "        \"Sodium\":Sodium,\n",
    "        \"Total_Carbohydrates\":Total_Carbohydrates,\n",
    "        \"Dietary_Fiber\":Dietary_Fiber,\n",
    "        \"Sugars\":Sugars,\n",
    "        \"Protein\":Protein,\n",
    "        \"Total_Fat_PDV\":Total_Fat_PDV,\n",
    "        \"Saturated_Fat_PDV\":Saturated_Fat_PDV,\n",
    "        \"Cholesterol_PDV\":Cholesterol_PDV,\n",
    "        \"Sodium_PDV\":Sodium_PDV,\n",
    "        \"Total_Carbohydrates_PDV\":Total_Carbohydrates_PDV,\n",
    "        \"Dietary_Fiber_PDV\":Dietary_Fiber_PDV,\n",
    "        \"Protein_PDV\":Protein_PDV,\n",
    "        \"Vitamin_A_PDV\":Vitamin_A_PDV,\n",
    "        \"Vitamin_C_PDV\":Vitamin_C_PDV,\n",
    "        \"Calcium_PDV\":Calcium_PDV,\n",
    "        \"Iron_PDV\":Iron_PDV,\n",
    "        \"Ingredients\":Ingredients,\n",
    "        \"Health_Rating\":Health_Rating,\n",
    "        \"Taste_Rating\":Taste_Rating\n",
    "    }\n",
    "    df = pd.DataFrame.from_dict(dictionary, orient ='index').transpose()\n",
    "\n",
    "    df.to_csv(\"allFastFood-nutriData\"+str(start)+\",\"+str(end)+\".csv\")\n",
    "    return (df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling [3178]https://fastfoodnutrition.org/starbucks/caffe-latte/choose-milk\n",
      "** Skipping invalid page\n",
      "Crawling [3179]https://fastfoodnutrition.org/starbucks/caffe-mocha/choose-milk\n",
      "** Skipping invalid page\n",
      "Crawling [3180]https://fastfoodnutrition.org/starbucks/cappuccino/choose-milk\n",
      "** Skipping invalid page\n",
      "Crawling [3181]https://fastfoodnutrition.org/starbucks/caramel-cloud-macchiato\n",
      "Further product list discovered- length:4\n",
      "error: has no data (pdv)-Vitamin A\n",
      "error: has no data (pdv)-Vitamin C\n",
      "error: has no data (pdv)-Calcium\n",
      "error: has no data (pdv)-Iron\n",
      "Success crawl... https://fastfoodnutrition.org/starbucks/caramel-cloud-macchiato/short\n",
      "error: has no data (pdv)-Vitamin A\n",
      "error: has no data (pdv)-Vitamin C\n",
      "error: has no data (pdv)-Calcium\n",
      "error: has no data (pdv)-Iron\n",
      "Success crawl... https://fastfoodnutrition.org/starbucks/caramel-cloud-macchiato/tall\n",
      "error: has no data (pdv)-Vitamin A\n",
      "error: has no data (pdv)-Vitamin C\n",
      "error: has no data (pdv)-Calcium\n",
      "error: has no data (pdv)-Iron\n",
      "Success crawl... https://fastfoodnutrition.org/starbucks/caramel-cloud-macchiato/grande\n",
      "error: has no data (pdv)-Vitamin A\n",
      "error: has no data (pdv)-Vitamin C\n",
      "error: has no data (pdv)-Calcium\n",
      "error: has no data (pdv)-Iron\n",
      "Success crawl... https://fastfoodnutrition.org/starbucks/caramel-cloud-macchiato/venti\n",
      "Crawling [3182]https://fastfoodnutrition.org/starbucks/caramel-macchiato/choose-milk\n",
      "** Skipping invalid page\n",
      "Crawling [3183]https://fastfoodnutrition.org/starbucks/cascara-latte/choose-milk\n",
      "** Skipping invalid page\n",
      "Crawling [3184]https://fastfoodnutrition.org/starbucks/cinnamon-dolce-latte/choose-milk\n",
      "** Skipping invalid page\n",
      "Crawling [3185]https://fastfoodnutrition.org/starbucks/cocoa-cloud-macchiato\n",
      "Further product list discovered- length:4\n",
      "error: has no data (pdv)-Vitamin A\n",
      "error: has no data (pdv)-Vitamin C\n",
      "error: has no data (pdv)-Calcium\n",
      "error: has no data (pdv)-Iron\n",
      "Success crawl... https://fastfoodnutrition.org/starbucks/cocoa-cloud-macchiato/short\n"
     ]
    }
   ],
   "source": [
    "# 3000,3500  because data is too big. better to split into ranges of data, then combine all\n",
    "    \n",
    "df=scrap_data(3178,3190)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
